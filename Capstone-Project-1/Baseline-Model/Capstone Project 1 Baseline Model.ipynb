{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my baseline model building part of my capstone project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('C:/Users/Hill HP 2015/Documents/Springboard/Capstone Project 1/cp1_data_clean.csv',index_col='Game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H Fenwick Close</th>\n",
       "      <th>H GF</th>\n",
       "      <th>H GA</th>\n",
       "      <th>H GlDiff</th>\n",
       "      <th>H PP%</th>\n",
       "      <th>H PK%</th>\n",
       "      <th>H sh%</th>\n",
       "      <th>H sv%</th>\n",
       "      <th>H PDO</th>\n",
       "      <th>H win streak</th>\n",
       "      <th>...</th>\n",
       "      <th>A GlDiff</th>\n",
       "      <th>A PP%</th>\n",
       "      <th>A PK%</th>\n",
       "      <th>A sh%</th>\n",
       "      <th>A sv%</th>\n",
       "      <th>A PDO</th>\n",
       "      <th>A win streak</th>\n",
       "      <th>A standing</th>\n",
       "      <th>A 5-5 F/A</th>\n",
       "      <th>Home Win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Game</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-16 Tampa Bay at Florida</th>\n",
       "      <td>46.73</td>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>-17</td>\n",
       "      <td>18.9</td>\n",
       "      <td>74.5</td>\n",
       "      <td>937</td>\n",
       "      <td>901</td>\n",
       "      <td>964</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>81.1</td>\n",
       "      <td>881</td>\n",
       "      <td>920</td>\n",
       "      <td>1039</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-16 Ottawa at Toronto</th>\n",
       "      <td>45.02</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>14.9</td>\n",
       "      <td>78.3</td>\n",
       "      <td>901</td>\n",
       "      <td>933</td>\n",
       "      <td>1032</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>17.6</td>\n",
       "      <td>89.3</td>\n",
       "      <td>932</td>\n",
       "      <td>945</td>\n",
       "      <td>1013</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-16 Philadelphia at Montreal</th>\n",
       "      <td>52.03</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>81.5</td>\n",
       "      <td>921</td>\n",
       "      <td>931</td>\n",
       "      <td>1010</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-8</td>\n",
       "      <td>18.3</td>\n",
       "      <td>80.6</td>\n",
       "      <td>933</td>\n",
       "      <td>911</td>\n",
       "      <td>978</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-16 New Jersey at NY Islanders</th>\n",
       "      <td>49.38</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>-6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>88.1</td>\n",
       "      <td>912</td>\n",
       "      <td>873</td>\n",
       "      <td>961</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>82.8</td>\n",
       "      <td>904</td>\n",
       "      <td>929</td>\n",
       "      <td>1025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-16 Anaheim at Nashville</th>\n",
       "      <td>46.55</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>16.3</td>\n",
       "      <td>85.2</td>\n",
       "      <td>924</td>\n",
       "      <td>947</td>\n",
       "      <td>1023</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>21.4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>870</td>\n",
       "      <td>933</td>\n",
       "      <td>1063</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       H Fenwick Close  H GF  H GA  H GlDiff  \\\n",
       "Game                                                                           \n",
       "2013-02-16 Tampa Bay at Florida                  46.73    30    47       -17   \n",
       "2013-02-16 Ottawa at Toronto                     45.02    40    36         4   \n",
       "2013-02-16 Philadelphia at Montreal              52.03    36    33         3   \n",
       "2013-02-16 New Jersey at NY Islanders            49.38    40    46        -6   \n",
       "2013-02-16 Anaheim at Nashville                  46.55    28    26         2   \n",
       "\n",
       "                                       H PP%  H PK%  H sh%  H sv%  H PDO  \\\n",
       "Game                                                                       \n",
       "2013-02-16 Tampa Bay at Florida         18.9   74.5    937    901    964   \n",
       "2013-02-16 Ottawa at Toronto            14.9   78.3    901    933   1032   \n",
       "2013-02-16 Philadelphia at Montreal     20.3   81.5    921    931   1010   \n",
       "2013-02-16 New Jersey at NY Islanders   25.0   88.1    912    873    961   \n",
       "2013-02-16 Anaheim at Nashville         16.3   85.2    924    947   1023   \n",
       "\n",
       "                                       H win streak    ...     A GlDiff  \\\n",
       "Game                                                   ...                \n",
       "2013-02-16 Tampa Bay at Florida                  -2    ...            9   \n",
       "2013-02-16 Ottawa at Toronto                     -1    ...            8   \n",
       "2013-02-16 Philadelphia at Montreal               2    ...           -8   \n",
       "2013-02-16 New Jersey at NY Islanders             1    ...            9   \n",
       "2013-02-16 Anaheim at Nashville                   2    ...           12   \n",
       "\n",
       "                                       A PP%  A PK%  A sh%  A sv%  A PDO  \\\n",
       "Game                                                                       \n",
       "2013-02-16 Tampa Bay at Florida         26.4   81.1    881    920   1039   \n",
       "2013-02-16 Ottawa at Toronto            17.6   89.3    932    945   1013   \n",
       "2013-02-16 Philadelphia at Montreal     18.3   80.6    933    911    978   \n",
       "2013-02-16 New Jersey at NY Islanders   20.7   82.8    904    929   1025   \n",
       "2013-02-16 Anaheim at Nashville         21.4   70.0    870    933   1063   \n",
       "\n",
       "                                       A win streak  A standing  A 5-5 F/A  \\\n",
       "Game                                                                         \n",
       "2013-02-16 Tampa Bay at Florida                  -1           9       1.28   \n",
       "2013-02-16 Ottawa at Toronto                     -1           7       1.35   \n",
       "2013-02-16 Philadelphia at Montreal              -1          10       0.85   \n",
       "2013-02-16 New Jersey at NY Islanders             1           1       1.29   \n",
       "2013-02-16 Anaheim at Nashville                   3           2       1.83   \n",
       "\n",
       "                                       Home Win  \n",
       "Game                                             \n",
       "2013-02-16 Tampa Bay at Florida               0  \n",
       "2013-02-16 Ottawa at Toronto                  1  \n",
       "2013-02-16 Philadelphia at Montreal           1  \n",
       "2013-02-16 New Jersey at NY Islanders         1  \n",
       "2013-02-16 Anaheim at Nashville               0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a recap, I'll examine the head for the dataset\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Home Win'],axis=1), \n",
    "                                              df['Home Win'],random_state=1,stratify=df['Home Win'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for L1 classifier at C 1e-07 is 0.415385\n",
      "Accuracy for L2 classifier at C 1e-07 is 0.584615\n",
      "Accuracy for L1 classifier at C 1e-06 is 0.415385\n",
      "Accuracy for L2 classifier at C 1e-06 is 0.584615\n",
      "Accuracy for L1 classifier at C 1e-05 is 0.415385\n",
      "Accuracy for L2 classifier at C 1e-05 is 0.553846\n",
      "Accuracy for L1 classifier at C 0.0001 is 0.584615\n",
      "Accuracy for L2 classifier at C 0.0001 is 0.515385\n",
      "Accuracy for L1 classifier at C 0.001 is 0.584615\n",
      "Accuracy for L2 classifier at C 0.001 is 0.546154\n",
      "Accuracy for L1 classifier at C 0.01 is 0.546154\n",
      "Accuracy for L2 classifier at C 0.01 is 0.538462\n",
      "Accuracy for L1 classifier at C 0.1 is 0.553846\n",
      "Accuracy for L2 classifier at C 0.1 is 0.561538\n",
      "Accuracy for L1 classifier at C 1 is 0.553846\n",
      "Accuracy for L2 classifier at C 1 is 0.561538\n",
      "Accuracy for L1 classifier at C 10 is 0.561538\n",
      "Accuracy for L2 classifier at C 10 is 0.561538\n",
      "Accuracy for L1 classifier at C 100 is 0.546154\n",
      "Accuracy for L2 classifier at C 100 is 0.561538\n",
      "Accuracy for L1 classifier at C 1000 is 0.546154\n",
      "Accuracy for L2 classifier at C 1000 is 0.561538\n",
      "Accuracy for L1 classifier at C 10000 is 0.553846\n",
      "Accuracy for L2 classifier at C 10000 is 0.561538\n",
      "Accuracy for L1 classifier at C 100000 is 0.546154\n",
      "Accuracy for L2 classifier at C 100000 is 0.561538\n",
      "Accuracy for L1 classifier at C 1000000 is 0.546154\n",
      "Accuracy for L2 classifier at C 1000000 is 0.561538\n",
      "Accuracy for L1 classifier at C 10000000 is 0.553846\n",
      "Accuracy for L2 classifier at C 10000000 is 0.553846\n",
      "\n",
      "Best accuracy for L1 classifier is 0.584615 at C 0.0001\n",
      "Best accuracy for L2 classifier is 0.584615 at C 1e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e-07, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Cs = [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01,  0.1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "l1_accuracy = np.empty(len(Cs))\n",
    "l2_accuracy = np.empty(len(Cs))\n",
    "\n",
    "for i in range(len(l1_accuracy)):\n",
    "    l1_accuracy[i] = 0\n",
    "    l2_accuracy[i] = 0\n",
    "\n",
    "count = 0\n",
    "max_l1 = 0\n",
    "max_l2 = 0\n",
    "for val in Cs:\n",
    "    clf_l1 = LogisticRegression(penalty='l1', C=val)\n",
    "    clf_l2 = LogisticRegression(penalty='l2', C=val)\n",
    "    clf_l1.fit(X_train,y_train)\n",
    "    clf_l2.fit(X_train,y_train)\n",
    "    l1_accuracy[count] = accuracy_score(clf_l1.predict(X_test), y_test)\n",
    "    if (l1_accuracy[count] > l1_accuracy[max_l1]):\n",
    "        max_l1 = count\n",
    "    l2_accuracy[count] = accuracy_score(clf_l2.predict(X_test), y_test)\n",
    "    if (l2_accuracy[count] > l2_accuracy[max_l2]):\n",
    "        max_l2 = count\n",
    "    print('Accuracy for L1 classifier at C {} is {:6f}'.format(val, l1_accuracy[count]))\n",
    "    print('Accuracy for L2 classifier at C {} is {:6f}'.format(val, l2_accuracy[count]))\n",
    "    count += 1\n",
    "    \n",
    "print('\\nBest accuracy for L1 classifier is {:6f} at C {}'.format(np.max(l1_accuracy),Cs[max_l1]))\n",
    "print('Best accuracy for L2 classifier is {:6f} at C {}'.format(np.max(l2_accuracy),Cs[max_l2]))\n",
    "\n",
    "clf_best_lr_l1 = LogisticRegression(penalty='l1', C=Cs[max_l1])\n",
    "clf_best_lr_l2 = LogisticRegression(penalty='l2', C=Cs[max_l2])\n",
    "clf_best_lr_l1.fit(X_train,y_train)\n",
    "clf_best_lr_l2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best L1 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       160\n",
      "          1       0.59      1.00      0.74       227\n",
      "\n",
      "avg / total       0.34      0.59      0.43       387\n",
      "\n",
      "[Final Test Classification Report Best L1 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        54\n",
      "          1       0.58      1.00      0.74        76\n",
      "\n",
      "avg / total       0.34      0.58      0.43       130\n",
      "\n",
      "[Final Training Classification Report Best L2 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       160\n",
      "          1       0.59      1.00      0.74       227\n",
      "\n",
      "avg / total       0.34      0.59      0.43       387\n",
      "\n",
      "[Final Test Classification Report Best L2 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        54\n",
      "          1       0.58      1.00      0.74        76\n",
      "\n",
      "avg / total       0.34      0.58      0.43       130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best L1 Regularization:]\")\n",
    "print(classification_report(y_train, clf_best_lr_l1.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best L1 Regularization:]\")\n",
    "print(classification_report(y_test, clf_best_lr_l1.predict(X_test)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best L2 Regularization:]\")\n",
    "print(classification_report(y_train, clf_best_lr_l2.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best L2 Regularization:]\")\n",
    "print(classification_report(y_test, clf_best_lr_l2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall scores for home team losses for both types of regularization are terrible.  I'll have to see if I can figure out what is causing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for L1 regularization:\n",
      "    estimated coefficients         features\n",
      "8                 0.000245            H PDO\n",
      "23                0.000000        A 5-5 F/A\n",
      "22                0.000000       A standing\n",
      "1                 0.000000             H GF\n",
      "2                 0.000000             H GA\n",
      "3                 0.000000         H GlDiff\n",
      "4                 0.000000            H PP%\n",
      "5                 0.000000            H PK%\n",
      "6                 0.000000            H sh%\n",
      "7                 0.000000            H sv%\n",
      "9                 0.000000     H win streak\n",
      "10                0.000000       H standing\n",
      "11                0.000000        H 5-5 F/A\n",
      "12                0.000000  A Fenwick Close\n",
      "13                0.000000             A GF\n",
      "14                0.000000             A GA\n",
      "15                0.000000         A GlDiff\n",
      "16                0.000000            A PP%\n",
      "17                0.000000            A PK%\n",
      "18                0.000000            A sh%\n",
      "19                0.000000            A sv%\n",
      "20                0.000000            A PDO\n",
      "21                0.000000     A win streak\n",
      "0                 0.000000  H Fenwick Close\n"
     ]
    }
   ],
   "source": [
    "df_coef_lr_l1 = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_lr_l1.coef_,24)})\n",
    "df_coef_lr_l1 = df_coef_lr_l1.reindex(df_coef_lr_l1['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for L1 regularization:')\n",
    "print(df_coef_lr_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for L2 regularization:\n",
      "    estimated coefficients         features\n",
      "8             8.084613e-05            H PDO\n",
      "7             6.991545e-05            H sv%\n",
      "18            6.855500e-05            A sh%\n",
      "6             5.785267e-05            H sh%\n",
      "20            4.745208e-05            A PDO\n",
      "19            4.722374e-05            A sv%\n",
      "3             3.647036e-05         H GlDiff\n",
      "5             1.120871e-05            H PK%\n",
      "22            1.086889e-05       A standing\n",
      "0             1.025682e-05  H Fenwick Close\n",
      "9             5.211837e-06     H win streak\n",
      "4             4.017041e-06            H PP%\n",
      "17            2.351048e-06            A PK%\n",
      "11            4.729917e-07        H 5-5 F/A\n",
      "12           -1.344825e-07  A Fenwick Close\n",
      "23           -3.042109e-07        A 5-5 F/A\n",
      "21           -5.661848e-07     A win streak\n",
      "16           -5.707842e-06            A PP%\n",
      "10           -6.855501e-06       H standing\n",
      "14           -7.309082e-06             A GA\n",
      "1            -1.822596e-05             H GF\n",
      "15           -4.220223e-05         A GlDiff\n",
      "13           -4.951131e-05             A GF\n",
      "2            -5.469633e-05             H GA\n"
     ]
    }
   ],
   "source": [
    "df_coef_lr_l2 = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_lr_l2.coef_,24)})\n",
    "df_coef_lr_l2 = df_coef_lr_l2.reindex(df_coef_lr_l2['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for L2 regularization:')\n",
    "print(df_coef_lr_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, for both regularizations, the estimated coefficients are also terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best L1 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.31      0.41       160\n",
      "          1       0.64      0.86      0.73       227\n",
      "\n",
      "avg / total       0.62      0.63      0.60       387\n",
      "\n",
      "[Final Test Classification Report Best L1 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.28      0.34        54\n",
      "          1       0.59      0.75      0.66        76\n",
      "\n",
      "avg / total       0.53      0.55      0.53       130\n",
      "\n",
      "[Final Training Classification Report Best L2 Regularization:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.31      0.41       160\n",
      "          1       0.64      0.86      0.73       227\n",
      "\n",
      "avg / total       0.63      0.63      0.60       387\n",
      "\n",
      "[Final Test Classification Report Best Logistic Regression:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.30      0.36        54\n",
      "          1       0.60      0.75      0.67        76\n",
      "\n",
      "avg / total       0.54      0.56      0.54       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# However, when using the best C values, both the classifiers' estimated coefficients were mostly zeroes or close to zero,\n",
    "# so I manually tested several C values and found that these provided the best results overall for each classifier\n",
    "\n",
    "max_l1 = 1000000\n",
    "max_l2 = 1000000\n",
    "\n",
    "clf_best_lr_l1 = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_best_lr_l2 = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_best_lr_l1.fit(X_train,y_train)\n",
    "clf_best_lr_l2.fit(X_train,y_train)\n",
    "\n",
    "print(\"[Final Training Classification Report Best L1 Regularization:]\")\n",
    "print(classification_report(y_train, clf_best_lr_l1.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best L1 Regularization:]\")\n",
    "print(classification_report(y_test, clf_best_lr_l1.predict(X_test)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best L2 Regularization:]\")\n",
    "print(classification_report(y_train, clf_best_lr_l2.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best Logistic Regression:]\")\n",
    "print(classification_report(y_test, clf_best_lr_l2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those scores are better, but still very low.  Time to check the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for L1 regularization:\n",
      "    estimated coefficients         features\n",
      "11                0.348112        H 5-5 F/A\n",
      "9                 0.062687     H win streak\n",
      "22                0.037380       A standing\n",
      "21                0.031342     A win streak\n",
      "12                0.030932  A Fenwick Close\n",
      "10                0.027438       H standing\n",
      "0                 0.023219  H Fenwick Close\n",
      "5                 0.017355            H PK%\n",
      "3                 0.011010         H GlDiff\n",
      "17                0.009707            A PK%\n",
      "4                 0.009428            H PP%\n",
      "20                0.007410            A PDO\n",
      "6                 0.001155            H sh%\n",
      "13               -0.000359             A GF\n",
      "19               -0.001039            A sv%\n",
      "14               -0.001143             A GA\n",
      "1                -0.001978             H GF\n",
      "2                -0.002101             H GA\n",
      "8                -0.002403            H PDO\n",
      "7                -0.003540            H sv%\n",
      "18               -0.008617            A sh%\n",
      "15               -0.021126         A GlDiff\n",
      "16               -0.027046            A PP%\n",
      "23               -0.117282        A 5-5 F/A\n"
     ]
    }
   ],
   "source": [
    "df_coef_lr_l1 = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_lr_l1.coef_,24)})\n",
    "df_coef_lr_l1 = df_coef_lr_l1.reindex(df_coef_lr_l1['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for L1 regularization:')\n",
    "print(df_coef_lr_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for L2 regularization:\n",
      "    estimated coefficients         features\n",
      "9                 0.064105     H win streak\n",
      "22                0.037798       A standing\n",
      "21                0.032886     A win streak\n",
      "12                0.029381  A Fenwick Close\n",
      "0                 0.028720  H Fenwick Close\n",
      "10                0.026412       H standing\n",
      "5                 0.015126            H PK%\n",
      "17                0.010823            A PK%\n",
      "3                 0.008567         H GlDiff\n",
      "20                0.007957            A PDO\n",
      "4                 0.007557            H PP%\n",
      "14                0.006323             A GA\n",
      "1                 0.002454             H GF\n",
      "11                0.002350        H 5-5 F/A\n",
      "6                 0.000306            H sh%\n",
      "23                0.000273        A 5-5 F/A\n",
      "8                -0.000891            H PDO\n",
      "19               -0.002146            A sv%\n",
      "7                -0.002845            H sv%\n",
      "2                -0.006113             H GA\n",
      "18               -0.007843            A sh%\n",
      "13               -0.008302             A GF\n",
      "15               -0.014625         A GlDiff\n",
      "16               -0.026389            A PP%\n"
     ]
    }
   ],
   "source": [
    "df_coef_lr_l2 = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_lr_l2.coef_,24)})\n",
    "df_coef_lr_l2 = df_coef_lr_l2.reindex(df_coef_lr_l2['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for L2 regularization:')\n",
    "print(df_coef_lr_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those look much better.  Time to try other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best oob score for sqrt classifier is 0.591731 at 72 estimators\n",
      "Best oob score for log2 classifier is 0.599483 at 81 estimators\n",
      "Best oob score for None classifier is 0.586563 at 74 estimators\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sr_accuracy = np.empty(91)\n",
    "lg_accuracy = np.empty(91)\n",
    "no_accuracy = np.empty(91)\n",
    "\n",
    "for i in range(len(sr_accuracy)):\n",
    "    sr_accuracy[i] = 0\n",
    "    lg_accuracy[i] = 0\n",
    "    no_accuracy[i] = 0\n",
    "\n",
    "count = 0\n",
    "max_sr = 0\n",
    "max_lg = 0\n",
    "max_no = 0\n",
    "for val in np.arange(10,100):\n",
    "    clf_sr = RandomForestClassifier(max_features='sqrt',n_estimators=val,oob_score=True)\n",
    "    clf_lg = RandomForestClassifier(max_features='log2',n_estimators=val,oob_score=True)\n",
    "    clf_no = RandomForestClassifier(max_features=None,n_estimators=val,oob_score=True)\n",
    "    clf_sr.fit(X_train,y_train)\n",
    "    clf_lg.fit(X_train,y_train)\n",
    "    clf_no.fit(X_train,y_train)\n",
    "    sr_accuracy[count] = clf_sr.oob_score_\n",
    "    if (sr_accuracy[count] > sr_accuracy[max_sr]):\n",
    "        max_sr = count\n",
    "    lg_accuracy[count] = clf_lg.oob_score_\n",
    "    if (lg_accuracy[count] > lg_accuracy[max_lg]):\n",
    "        max_lg = count\n",
    "    no_accuracy[count] = clf_no.oob_score_\n",
    "    if (no_accuracy[count] > no_accuracy[max_no]):\n",
    "        max_no = count\n",
    "    #print('Accuracy for sqrt classifier at C {} is {:6f}'.format(val, sr_accuracy[count]))\n",
    "    #print('Accuracy for log2 classifier at C {} is {:6f}'.format(val, lg_accuracy[count]))\n",
    "    #print('Accuracy for None classifier at C {} is {:6f}'.format(val, no_accuracy[count]))\n",
    "    count += 1\n",
    "    \n",
    "print('Best oob score for sqrt classifier is {:6f} at {} estimators'.format(np.max(sr_accuracy),max_sr))\n",
    "print('Best oob score for log2 classifier is {:6f} at {} estimators'.format(np.max(lg_accuracy),max_lg))\n",
    "print('Best oob score for None classifier is {:6f} at {} estimators'.format(np.max(no_accuracy),max_no))\n",
    "\n",
    "clf_best_rf_sr = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_best_rf_lg = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_best_rf_no = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_best_rf_sr.fit(X_train,y_train)\n",
    "clf_best_rf_lg.fit(X_train,y_train)\n",
    "clf_best_rf_no.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best sqrt Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       160\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       387\n",
      "\n",
      "[Final Test Classification Report Best sqrt Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.44      0.46        54\n",
      "          1       0.62      0.64      0.63        76\n",
      "\n",
      "avg / total       0.56      0.56      0.56       130\n",
      "\n",
      "[Final Training Classification Report Best log2 Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       160\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       387\n",
      "\n",
      "[Final Test Classification Report Best log2 Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.37      0.41        54\n",
      "          1       0.60      0.68      0.64        76\n",
      "\n",
      "avg / total       0.54      0.55      0.54       130\n",
      "\n",
      "[Final Training Classification Report Best None Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       160\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       387\n",
      "\n",
      "[Final Test Classification Report Best Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.33      0.36        54\n",
      "          1       0.57      0.63      0.60        76\n",
      "\n",
      "avg / total       0.50      0.51      0.50       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best sqrt Random Forest:]\")\n",
    "print(classification_report(y_train, clf_best_rf_sr.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best sqrt Random Forest:]\")\n",
    "print(classification_report(y_test, clf_best_rf_sr.predict(X_test)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best log2 Random Forest:]\")\n",
    "print(classification_report(y_train, clf_best_rf_lg.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best log2 Random Forest:]\")\n",
    "print(classification_report(y_test, clf_best_rf_lg.predict(X_test)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best None Random Forest:]\")\n",
    "print(classification_report(y_train, clf_best_rf_no.predict(X_train)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best Random Forest:]\")\n",
    "print(classification_report(y_test, clf_best_rf_no.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test scores look similar to the logistic regression scores.  Looks like there is still room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for sqrt trees:\n",
      "    estimated coefficients         features\n",
      "16                0.055766            A PP%\n",
      "12                0.051189  A Fenwick Close\n",
      "4                 0.050735            H PP%\n",
      "1                 0.049783             H GF\n",
      "2                 0.049543             H GA\n",
      "0                 0.047374  H Fenwick Close\n",
      "17                0.046622            A PK%\n",
      "23                0.045855        A 5-5 F/A\n",
      "20                0.045059            A PDO\n",
      "14                0.044780             A GA\n",
      "13                0.042027             A GF\n",
      "6                 0.041994            H sh%\n",
      "22                0.041975       A standing\n",
      "15                0.040958         A GlDiff\n",
      "19                0.039828            A sv%\n",
      "7                 0.039696            H sv%\n",
      "8                 0.039600            H PDO\n",
      "5                 0.038353            H PK%\n",
      "18                0.037192            A sh%\n",
      "11                0.036669        H 5-5 F/A\n",
      "3                 0.036396         H GlDiff\n",
      "10                0.028246       H standing\n",
      "21                0.026140     A win streak\n",
      "9                 0.024220     H win streak\n"
     ]
    }
   ],
   "source": [
    "df_coef_rf_sr = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_rf_sr.feature_importances_,24)})\n",
    "df_coef_rf_sr = df_coef_rf_sr.reindex(df_coef_rf_sr['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for sqrt trees:')\n",
    "print(df_coef_rf_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for log2 trees:\n",
      "    estimated coefficients         features\n",
      "17                0.057984            A PK%\n",
      "4                 0.051924            H PP%\n",
      "16                0.051770            A PP%\n",
      "0                 0.049906  H Fenwick Close\n",
      "2                 0.048749             H GA\n",
      "1                 0.048100             H GF\n",
      "14                0.047537             A GA\n",
      "13                0.045921             A GF\n",
      "5                 0.045155            H PK%\n",
      "20                0.044998            A PDO\n",
      "12                0.043469  A Fenwick Close\n",
      "19                0.042758            A sv%\n",
      "15                0.042433         A GlDiff\n",
      "18                0.042339            A sh%\n",
      "23                0.041735        A 5-5 F/A\n",
      "6                 0.039745            H sh%\n",
      "8                 0.036534            H PDO\n",
      "7                 0.035427            H sv%\n",
      "3                 0.034550         H GlDiff\n",
      "11                0.034517        H 5-5 F/A\n",
      "22                0.030189       A standing\n",
      "9                 0.029138     H win streak\n",
      "10                0.027942       H standing\n",
      "21                0.027180     A win streak\n"
     ]
    }
   ],
   "source": [
    "df_coef_rf_lg = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_rf_lg.feature_importances_,24)})\n",
    "df_coef_rf_lg = df_coef_rf_lg.reindex(df_coef_rf_lg['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for log2 trees:')\n",
    "print(df_coef_rf_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for None trees:\n",
      "    estimated coefficients         features\n",
      "16                0.065051            A PP%\n",
      "17                0.060591            A PK%\n",
      "12                0.055819  A Fenwick Close\n",
      "2                 0.055715             H GA\n",
      "0                 0.053044  H Fenwick Close\n",
      "5                 0.051834            H PK%\n",
      "4                 0.051357            H PP%\n",
      "1                 0.047083             H GF\n",
      "20                0.045215            A PDO\n",
      "13                0.044887             A GF\n",
      "23                0.040961        A 5-5 F/A\n",
      "6                 0.040903            H sh%\n",
      "19                0.040891            A sv%\n",
      "18                0.039127            A sh%\n",
      "7                 0.037815            H sv%\n",
      "15                0.034955         A GlDiff\n",
      "8                 0.033794            H PDO\n",
      "14                0.031108             A GA\n",
      "3                 0.030353         H GlDiff\n",
      "22                0.029933       A standing\n",
      "9                 0.029723     H win streak\n",
      "11                0.029324        H 5-5 F/A\n",
      "21                0.028869     A win streak\n",
      "10                0.021650       H standing\n"
     ]
    }
   ],
   "source": [
    "df_coef_rf_no = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_best_rf_no.feature_importances_,24)})\n",
    "df_coef_rf_no = df_coef_rf_no.reindex(df_coef_rf_no['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for None trees:')\n",
    "print(df_coef_rf_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H Fenwick Close</th>\n",
       "      <th>H GF</th>\n",
       "      <th>H GA</th>\n",
       "      <th>H GlDiff</th>\n",
       "      <th>H PP%</th>\n",
       "      <th>H PK%</th>\n",
       "      <th>H sh%</th>\n",
       "      <th>H sv%</th>\n",
       "      <th>H PDO</th>\n",
       "      <th>H win streak</th>\n",
       "      <th>...</th>\n",
       "      <th>A sh%</th>\n",
       "      <th>A sv%</th>\n",
       "      <th>A PDO</th>\n",
       "      <th>A win streak</th>\n",
       "      <th>A standing</th>\n",
       "      <th>A 5-5 F/A</th>\n",
       "      <th>A Rest</th>\n",
       "      <th>A Travel</th>\n",
       "      <th>A Travel per Day</th>\n",
       "      <th>Home Win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Game</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-08 Anaheim at Dallas</th>\n",
       "      <td>48.44</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>-5</td>\n",
       "      <td>13.9</td>\n",
       "      <td>83.3</td>\n",
       "      <td>930</td>\n",
       "      <td>940</td>\n",
       "      <td>1010</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>878</td>\n",
       "      <td>933</td>\n",
       "      <td>1055</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.024662</td>\n",
       "      <td>2</td>\n",
       "      <td>663</td>\n",
       "      <td>331.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-09 Pittsburgh at New Jersey</th>\n",
       "      <td>51.87</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87.8</td>\n",
       "      <td>914</td>\n",
       "      <td>926</td>\n",
       "      <td>1012</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>909</td>\n",
       "      <td>924</td>\n",
       "      <td>1015</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.024662</td>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-09 Carolina at Philadelphia</th>\n",
       "      <td>52.68</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>-4</td>\n",
       "      <td>18.4</td>\n",
       "      <td>76.0</td>\n",
       "      <td>935</td>\n",
       "      <td>940</td>\n",
       "      <td>1005</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>935</td>\n",
       "      <td>930</td>\n",
       "      <td>995</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.024662</td>\n",
       "      <td>2</td>\n",
       "      <td>379</td>\n",
       "      <td>189.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-09 Winnipeg at Ottawa</th>\n",
       "      <td>56.28</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.1</td>\n",
       "      <td>924</td>\n",
       "      <td>942</td>\n",
       "      <td>1018</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>926</td>\n",
       "      <td>900</td>\n",
       "      <td>974</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.024662</td>\n",
       "      <td>2</td>\n",
       "      <td>1042</td>\n",
       "      <td>521.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-09 Edmonton at Detroit</th>\n",
       "      <td>54.49</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>-2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>69.6</td>\n",
       "      <td>922</td>\n",
       "      <td>937</td>\n",
       "      <td>1015</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>954</td>\n",
       "      <td>921</td>\n",
       "      <td>967</td>\n",
       "      <td>-2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.024662</td>\n",
       "      <td>3</td>\n",
       "      <td>1593</td>\n",
       "      <td>531.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     H Fenwick Close  H GF  H GA  H GlDiff  \\\n",
       "Game                                                                         \n",
       "2013-02-08 Anaheim at Dallas                   48.44    22    27        -5   \n",
       "2013-02-09 Pittsburgh at New Jersey            51.87    27    21         6   \n",
       "2013-02-09 Carolina at Philadelphia            52.68    25    29        -4   \n",
       "2013-02-09 Winnipeg at Ottawa                  56.28    31    21        10   \n",
       "2013-02-09 Edmonton at Detroit                 54.49    27    29        -2   \n",
       "\n",
       "                                     H PP%  H PK%  H sh%  H sv%  H PDO  \\\n",
       "Game                                                                     \n",
       "2013-02-08 Anaheim at Dallas          13.9   83.3    930    940   1010   \n",
       "2013-02-09 Pittsburgh at New Jersey   19.0   87.8    914    926   1012   \n",
       "2013-02-09 Carolina at Philadelphia   18.4   76.0    935    940   1005   \n",
       "2013-02-09 Winnipeg at Ottawa         25.0   89.1    924    942   1018   \n",
       "2013-02-09 Edmonton at Detroit        12.0   69.6    922    937   1015   \n",
       "\n",
       "                                     H win streak    ...     A sh%  A sv%  \\\n",
       "Game                                                 ...                    \n",
       "2013-02-08 Anaheim at Dallas                    2    ...       878    933   \n",
       "2013-02-09 Pittsburgh at New Jersey             3    ...       909    924   \n",
       "2013-02-09 Carolina at Philadelphia            -1    ...       935    930   \n",
       "2013-02-09 Winnipeg at Ottawa                  -1    ...       926    900   \n",
       "2013-02-09 Edmonton at Detroit                  1    ...       954    921   \n",
       "\n",
       "                                     A PDO  A win streak  A standing  \\\n",
       "Game                                                                   \n",
       "2013-02-08 Anaheim at Dallas          1055             4           2   \n",
       "2013-02-09 Pittsburgh at New Jersey   1015             5           1   \n",
       "2013-02-09 Carolina at Philadelphia    995             2           8   \n",
       "2013-02-09 Winnipeg at Ottawa          974            -1          11   \n",
       "2013-02-09 Edmonton at Detroit         967            -2           8   \n",
       "\n",
       "                                     A 5-5 F/A  A Rest  A Travel  \\\n",
       "Game                                                               \n",
       "2013-02-08 Anaheim at Dallas          1.024662       2       663   \n",
       "2013-02-09 Pittsburgh at New Jersey   1.024662       2       306   \n",
       "2013-02-09 Carolina at Philadelphia   1.024662       2       379   \n",
       "2013-02-09 Winnipeg at Ottawa         1.024662       2      1042   \n",
       "2013-02-09 Edmonton at Detroit        1.024662       3      1593   \n",
       "\n",
       "                                     A Travel per Day  Home Win  \n",
       "Game                                                             \n",
       "2013-02-08 Anaheim at Dallas                    331.5         1  \n",
       "2013-02-09 Pittsburgh at New Jersey             153.0         1  \n",
       "2013-02-09 Carolina at Philadelphia             189.5         1  \n",
       "2013-02-09 Winnipeg at Ottawa                   521.0         0  \n",
       "2013-02-09 Edmonton at Detroit                  531.0         1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to train logistic regression classifiers with the data the original authors didn't use and the extra data I entered\n",
    "\n",
    "dfex = pd.read_csv('C:/Users/Hill HP 2015/Documents/Springboard/Capstone Project 1/cp1_data_clean_with_na.csv',index_col='Game')\n",
    "dfex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(dfex.drop(['Home Win'],axis=1), \n",
    "                                              dfex['Home Win'],random_state=1,stratify=dfex['Home Win'])\n",
    "\n",
    "clf_lr_l1_ex = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_lr_l1_ex.fit(X_train_ex,y_train_ex)\n",
    "clf_lr_l2_ex = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_lr_l2_ex.fit(X_train_ex,y_train_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best L1 Regularization with extra data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.39      0.47       182\n",
      "          1       0.64      0.81      0.71       244\n",
      "\n",
      "avg / total       0.62      0.63      0.61       426\n",
      "\n",
      "[Final Test Classification Report Best L1 Regularization with extra data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.38      0.42        61\n",
      "          1       0.60      0.69      0.64        81\n",
      "\n",
      "avg / total       0.55      0.56      0.55       142\n",
      "\n",
      "[Final Training Classification Report Best L2 Regularization with extra data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.38      0.47       182\n",
      "          1       0.64      0.82      0.72       244\n",
      "\n",
      "avg / total       0.63      0.63      0.61       426\n",
      "\n",
      "[Final Test Classification Report Best L2 Regularization with extra data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.34      0.39        61\n",
      "          1       0.58      0.69      0.63        81\n",
      "\n",
      "avg / total       0.53      0.54      0.53       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best L1 Regularization with extra data:]\")\n",
    "print(classification_report(y_train_ex, clf_lr_l1_ex.predict(X_train_ex)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best L1 Regularization with extra data:]\")\n",
    "print(classification_report(y_test_ex, clf_lr_l1_ex.predict(X_test_ex)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best L2 Regularization with extra data:]\")\n",
    "print(classification_report(y_train_ex, clf_lr_l2_ex.predict(X_train_ex)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best L2 Regularization with extra data:]\")\n",
    "print(classification_report(y_test_ex, clf_lr_l2_ex.predict(X_test_ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_sr_ex = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_rf_lg_ex = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_rf_no_ex = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_rf_sr_ex.fit(X_train_ex,y_train_ex)\n",
    "clf_rf_lg_ex.fit(X_train_ex,y_train_ex)\n",
    "clf_rf_no_ex.fit(X_train_ex,y_train_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best sqrt Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       182\n",
      "          1       1.00      1.00      1.00       244\n",
      "\n",
      "avg / total       1.00      1.00      1.00       426\n",
      "\n",
      "[Final Test Classification Report Best sqrt Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.31      0.38        61\n",
      "          1       0.59      0.75      0.66        81\n",
      "\n",
      "avg / total       0.55      0.56      0.54       142\n",
      "\n",
      "[Final Training Classification Report Best log2 Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       182\n",
      "          1       1.00      1.00      1.00       244\n",
      "\n",
      "avg / total       1.00      1.00      1.00       426\n",
      "\n",
      "[Final Test Classification Report Best log2 Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.30      0.37        61\n",
      "          1       0.59      0.77      0.67        81\n",
      "\n",
      "avg / total       0.55      0.56      0.54       142\n",
      "\n",
      "[Final Training Classification Report Best None Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       182\n",
      "          1       1.00      1.00      1.00       244\n",
      "\n",
      "avg / total       1.00      1.00      1.00       426\n",
      "\n",
      "[Final Test Classification Report Best None Random Forest:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.33      0.38        61\n",
      "          1       0.58      0.69      0.63        81\n",
      "\n",
      "avg / total       0.52      0.54      0.52       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best sqrt Random Forest:]\")\n",
    "print(classification_report(y_train_ex, clf_rf_sr_ex.predict(X_train_ex)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best sqrt Random Forest:]\")\n",
    "print(classification_report(y_test_ex, clf_rf_sr_ex.predict(X_test_ex)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best log2 Random Forest:]\")\n",
    "print(classification_report(y_train_ex, clf_rf_lg_ex.predict(X_train_ex)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best log2 Random Forest:]\")\n",
    "print(classification_report(y_test_ex, clf_rf_lg_ex.predict(X_test_ex)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best None Random Forest:]\")\n",
    "print(classification_report(y_train_ex, clf_rf_no_ex.predict(X_train_ex)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best None Random Forest:]\")\n",
    "print(classification_report(y_test_ex, clf_rf_no_ex.predict(X_test_ex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even using the extra data doesn't produce anything better.  What about a different size training split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hill HP 2015\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_70, X_test_70, y_train_70, y_test_70 = train_test_split(df.drop(['Home Win'],axis=1), \n",
    "                                              df['Home Win'],random_state=1,stratify=df['Home Win'],train_size=0.7)\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(df.drop(['Home Win'],axis=1), \n",
    "                                              df['Home Win'],random_state=1,stratify=df['Home Win'],train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_l1_70 = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_lr_l1_70.fit(X_train_70,y_train_70)\n",
    "clf_lr_l2_70 = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_lr_l2_70.fit(X_train_70,y_train_70)\n",
    "clf_lr_l1_80 = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_lr_l1_80.fit(X_train_80,y_train_80)\n",
    "clf_lr_l2_80 = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_lr_l2_80.fit(X_train_80,y_train_80)\n",
    "clf_rf_sr_70 = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_rf_sr_70.fit(X_train_70, y_train_70)\n",
    "clf_rf_lg_70 = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_rf_lg_70.fit(X_train_70, y_train_70)\n",
    "clf_rf_no_70 = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_rf_no_70.fit(X_train_70, y_train_70)\n",
    "clf_rf_sr_80 = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_rf_sr_80.fit(X_train_80, y_train_80)\n",
    "clf_rf_lg_80 = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_rf_lg_80.fit(X_train_80, y_train_80)\n",
    "clf_rf_no_80 = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_rf_no_80.fit(X_train_80, y_train_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report L1 Regularization 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.34      0.43       149\n",
      "          1       0.65      0.85      0.74       212\n",
      "\n",
      "avg / total       0.63      0.64      0.61       361\n",
      "\n",
      "[Final Test Classification Report L1 Regularization 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.26      0.32        65\n",
      "          1       0.58      0.74      0.65        91\n",
      "\n",
      "avg / total       0.51      0.54      0.51       156\n",
      "\n",
      "[Final Training Classification Report L1 Regularization 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.33      0.43       171\n",
      "          1       0.64      0.84      0.73       242\n",
      "\n",
      "avg / total       0.62      0.63      0.60       413\n",
      "\n",
      "[Final Test Classification Report L1 Regularization 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.26      0.31        43\n",
      "          1       0.58      0.74      0.65        61\n",
      "\n",
      "avg / total       0.51      0.54      0.51       104\n",
      "\n",
      "[Final Training Classification Report L2 Regularization 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.32      0.42       149\n",
      "          1       0.64      0.84      0.73       212\n",
      "\n",
      "avg / total       0.62      0.63      0.60       361\n",
      "\n",
      "[Final Test Classification Report L2 Regularization 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.28      0.34        65\n",
      "          1       0.59      0.75      0.66        91\n",
      "\n",
      "avg / total       0.53      0.55      0.53       156\n",
      "\n",
      "[Final Training Classification Report L2 Regularization 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.32      0.42       171\n",
      "          1       0.64      0.84      0.73       242\n",
      "\n",
      "avg / total       0.62      0.63      0.60       413\n",
      "\n",
      "[Final Test Classification Report L2 Regularization 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.26      0.32        43\n",
      "          1       0.59      0.75      0.66        61\n",
      "\n",
      "avg / total       0.52      0.55      0.52       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report L1 Regularization 70% train:]\")\n",
    "print(classification_report(y_train_70, clf_lr_l1_70.predict(X_train_70)))\n",
    "\n",
    "print(\"[Final Test Classification Report L1 Regularization 70% train:]\")\n",
    "print(classification_report(y_test_70, clf_lr_l1_70.predict(X_test_70)))\n",
    "\n",
    "print(\"[Final Training Classification Report L1 Regularization 80% train:]\")\n",
    "print(classification_report(y_train_80, clf_lr_l1_80.predict(X_train_80)))\n",
    "\n",
    "print(\"[Final Test Classification Report L1 Regularization 80% train:]\")\n",
    "print(classification_report(y_test_80, clf_lr_l1_80.predict(X_test_80)))\n",
    "\n",
    "print(\"[Final Training Classification Report L2 Regularization 70% train:]\")\n",
    "print(classification_report(y_train_70, clf_lr_l2_70.predict(X_train_70)))\n",
    "\n",
    "print(\"[Final Test Classification Report L2 Regularization 70% train:]\")\n",
    "print(classification_report(y_test_70, clf_lr_l2_70.predict(X_test_70)))\n",
    "\n",
    "print(\"[Final Training Classification Report L2 Regularization 80% train:]\")\n",
    "print(classification_report(y_train_80, clf_lr_l2_80.predict(X_train_80)))\n",
    "\n",
    "print(\"[Final Test Classification Report L2 Regularization 80% train:]\")\n",
    "print(classification_report(y_test_80, clf_lr_l2_80.predict(X_test_80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report sqrt Random Forest 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       149\n",
      "          1       1.00      1.00      1.00       212\n",
      "\n",
      "avg / total       1.00      1.00      1.00       361\n",
      "\n",
      "[Final Test Classification Report sqrt Random Forest 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.29      0.35        65\n",
      "          1       0.59      0.73      0.65        91\n",
      "\n",
      "avg / total       0.52      0.54      0.52       156\n",
      "\n",
      "[Final Training Classification Report sqrt Random Forest 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       171\n",
      "          1       1.00      1.00      1.00       242\n",
      "\n",
      "avg / total       1.00      1.00      1.00       413\n",
      "\n",
      "[Final Test Classification Report sqrt Random Forest 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.40      0.43        43\n",
      "          1       0.62      0.69      0.65        61\n",
      "\n",
      "avg / total       0.56      0.57      0.56       104\n",
      "\n",
      "[Final Training Classification Report log2 Random Forest 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       149\n",
      "          1       1.00      1.00      1.00       212\n",
      "\n",
      "avg / total       1.00      1.00      1.00       361\n",
      "\n",
      "[Final Test Classification Report log2 Random Forest 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.31      0.36        65\n",
      "          1       0.59      0.73      0.65        91\n",
      "\n",
      "avg / total       0.53      0.55      0.53       156\n",
      "\n",
      "[Final Training Classification Report log2 Random Forest 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       171\n",
      "          1       1.00      1.00      1.00       242\n",
      "\n",
      "avg / total       1.00      1.00      1.00       413\n",
      "\n",
      "[Final Test Classification Report log2 Random Forest 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.26      0.31        43\n",
      "          1       0.58      0.74      0.65        61\n",
      "\n",
      "avg / total       0.51      0.54      0.51       104\n",
      "\n",
      "[Final Training Classification Report None Random Forest 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       149\n",
      "          1       1.00      1.00      1.00       212\n",
      "\n",
      "avg / total       1.00      1.00      1.00       361\n",
      "\n",
      "[Final Test Classification Report None Random Forest 70% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.46      0.48        65\n",
      "          1       0.64      0.68      0.66        91\n",
      "\n",
      "avg / total       0.58      0.59      0.59       156\n",
      "\n",
      "[Final Training Classification Report None Random Forest 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       171\n",
      "          1       1.00      1.00      1.00       242\n",
      "\n",
      "avg / total       1.00      1.00      1.00       413\n",
      "\n",
      "[Final Test Classification Report None Random Forest 80% train:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.42      0.46        43\n",
      "          1       0.64      0.72      0.68        61\n",
      "\n",
      "avg / total       0.59      0.60      0.59       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report sqrt Random Forest 70% train:]\")\n",
    "print(classification_report(y_train_70, clf_rf_sr_70.predict(X_train_70)))\n",
    "\n",
    "print(\"[Final Test Classification Report sqrt Random Forest 70% train:]\")\n",
    "print(classification_report(y_test_70, clf_rf_sr_70.predict(X_test_70)))\n",
    "\n",
    "print(\"[Final Training Classification Report sqrt Random Forest 80% train:]\")\n",
    "print(classification_report(y_train_80, clf_rf_sr_80.predict(X_train_80)))\n",
    "\n",
    "print(\"[Final Test Classification Report sqrt Random Forest 80% train:]\")\n",
    "print(classification_report(y_test_80, clf_rf_sr_80.predict(X_test_80)))\n",
    "\n",
    "print(\"[Final Training Classification Report log2 Random Forest 70% train:]\")\n",
    "print(classification_report(y_train_70, clf_rf_lg_70.predict(X_train_70)))\n",
    "\n",
    "print(\"[Final Test Classification Report log2 Random Forest 70% train:]\")\n",
    "print(classification_report(y_test_70, clf_rf_lg_70.predict(X_test_70)))\n",
    "\n",
    "print(\"[Final Training Classification Report log2 Random Forest 80% train:]\")\n",
    "print(classification_report(y_train_80, clf_rf_lg_80.predict(X_train_80)))\n",
    "\n",
    "print(\"[Final Test Classification Report log2 Random Forest 80% train:]\")\n",
    "print(classification_report(y_test_80, clf_rf_lg_80.predict(X_test_80)))\n",
    "\n",
    "print(\"[Final Training Classification Report None Random Forest 70% train:]\")\n",
    "print(classification_report(y_train_70, clf_rf_no_70.predict(X_train_70)))\n",
    "\n",
    "print(\"[Final Test Classification Report None Random Forest 70% train:]\")\n",
    "print(classification_report(y_test_70, clf_rf_no_70.predict(X_test_70)))\n",
    "\n",
    "print(\"[Final Training Classification Report None Random Forest 80% train:]\")\n",
    "print(classification_report(y_train_80, clf_rf_no_80.predict(X_train_80)))\n",
    "\n",
    "print(\"[Final Test Classification Report None Random Forest 80% train:]\")\n",
    "print(classification_report(y_test_80, clf_rf_no_80.predict(X_test_80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no real difference.  Maybe some resampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_oversampled, y_oversampled = ros.fit_sample(df.drop(['Home Win'],axis=1), df['Home Win'])\n",
    "\n",
    "df_over = pd.DataFrame(X_oversampled)\n",
    "df_over['Home Win'] = y_oversampled\n",
    "\n",
    "X_train_o, X_test_o, y_train_o, y_test_o = train_test_split(df_over.drop(['Home Win'],axis=1), \n",
    "                                              df_over['Home Win'],random_state=1,stratify=df_over['Home Win'])\n",
    "\n",
    "clf_l1_over = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_l1_over.fit(X_train_o,y_train_o)\n",
    "clf_l2_over = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_l2_over.fit(X_train_o,y_train_o)\n",
    "clf_sr_over = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_sr_over.fit(X_train_o,y_train_o)\n",
    "clf_lg_over = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_lg_over.fit(X_train_o,y_train_o)\n",
    "clf_no_over = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_no_over.fit(X_train_o,y_train_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Train Classification Report L1 Regularization naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.63      0.62       227\n",
      "          1       0.62      0.60      0.61       227\n",
      "\n",
      "avg / total       0.61      0.61      0.61       454\n",
      "\n",
      "[Final Test Classification Report L1 Regularization naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.66      0.61        76\n",
      "          1       0.59      0.50      0.54        76\n",
      "\n",
      "avg / total       0.58      0.58      0.58       152\n",
      "\n",
      "[Final Train Classification Report L2 Regularization naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.63      0.62       227\n",
      "          1       0.62      0.61      0.62       227\n",
      "\n",
      "avg / total       0.62      0.62      0.62       454\n",
      "\n",
      "[Final Test Classification Report L2 Regularization naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.64      0.60        76\n",
      "          1       0.59      0.51      0.55        76\n",
      "\n",
      "avg / total       0.58      0.58      0.58       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Train Classification Report L1 Regularization naive over-resampled data:]\")\n",
    "print(classification_report(y_train_o, clf_l1_over.predict(X_train_o)))\n",
    "\n",
    "print(\"[Final Test Classification Report L1 Regularization naive over-resampled data:]\")\n",
    "print(classification_report(y_test_o, clf_l1_over.predict(X_test_o)))\n",
    "\n",
    "print(\"[Final Train Classification Report L2 Regularization naive over-resampled data:]\")\n",
    "print(classification_report(y_train_o, clf_l2_over.predict(X_train_o)))\n",
    "\n",
    "print(\"[Final Test Classification Report L2 Regularization naive over-resampled data:]\")\n",
    "print(classification_report(y_test_o, clf_l2_over.predict(X_test_o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best sqrt Random Forest naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       227\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       454\n",
      "\n",
      "[Final Test Classification Report Best sqrt Random Forest naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.72      0.66        76\n",
      "          1       0.66      0.54      0.59        76\n",
      "\n",
      "avg / total       0.64      0.63      0.63       152\n",
      "\n",
      "[Final Training Classification Report Best log2 Random Forest naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       227\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       454\n",
      "\n",
      "[Final Test Classification Report Best log2 Random Forest naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.71      0.67        76\n",
      "          1       0.67      0.59      0.63        76\n",
      "\n",
      "avg / total       0.65      0.65      0.65       152\n",
      "\n",
      "[Final Training Classification Report Best None Random Forest naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       227\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       454\n",
      "\n",
      "[Final Test Classification Report Best None Random Forest naive over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.79      0.69        76\n",
      "          1       0.70      0.49      0.57        76\n",
      "\n",
      "avg / total       0.65      0.64      0.63       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best sqrt Random Forest naive over-resampled data:]\")\n",
    "print(classification_report(y_train_o, clf_sr_over.predict(X_train_o)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best sqrt Random Forest naive over-resampled data:]\")\n",
    "print(classification_report(y_test_o, clf_sr_over.predict(X_test_o)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best log2 Random Forest naive over-resampled data:]\")\n",
    "print(classification_report(y_train_o, clf_lg_over.predict(X_train_o)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best log2 Random Forest naive over-resampled data:]\")\n",
    "print(classification_report(y_test_o, clf_lg_over.predict(X_test_o)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best None Random Forest naive over-resampled data:]\")\n",
    "print(classification_report(y_train_o, clf_no_over.predict(X_train_o)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best None Random Forest naive over-resampled data:]\")\n",
    "print(classification_report(y_test_o, clf_no_over.predict(X_test_o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_undersampled, y_undersampled = rus.fit_sample(df.drop(['Home Win'],axis=1), df['Home Win'])\n",
    "\n",
    "df_under = pd.DataFrame(X_undersampled)\n",
    "df_under['Home Win'] = y_undersampled\n",
    "\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(df_under.drop(['Home Win'],axis=1), \n",
    "                                              df_under['Home Win'],random_state=1,stratify=df_under['Home Win'])\n",
    "\n",
    "clf_l1_under = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_l1_under.fit(X_train_u,y_train_u)\n",
    "clf_l2_under = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_l2_under.fit(X_train_u,y_train_u)\n",
    "clf_sr_under = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_sr_under.fit(X_train_u,y_train_u)\n",
    "clf_lg_under = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_lg_under.fit(X_train_u,y_train_u)\n",
    "clf_no_under = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_no_under.fit(X_train_u,y_train_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Train Classification Report L1 Regularization under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.58      0.58       161\n",
      "          1       0.58      0.58      0.58       160\n",
      "\n",
      "avg / total       0.58      0.58      0.58       321\n",
      "\n",
      "[Final Test Classification Report L1 Regularization under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.58      0.58        53\n",
      "          1       0.59      0.59      0.59        54\n",
      "\n",
      "avg / total       0.59      0.59      0.59       107\n",
      "\n",
      "[Final Train Classification Report L2 Regularization under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.57      0.57       161\n",
      "          1       0.57      0.57      0.57       160\n",
      "\n",
      "avg / total       0.57      0.57      0.57       321\n",
      "\n",
      "[Final Test Classification Report L2 Regularization under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.57      0.58        53\n",
      "          1       0.59      0.61      0.60        54\n",
      "\n",
      "avg / total       0.59      0.59      0.59       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Train Classification Report L1 Regularization under-resampled data:]\")\n",
    "print(classification_report(y_train_u, clf_l1_under.predict(X_train_u)))\n",
    "\n",
    "print(\"[Final Test Classification Report L1 Regularization under-resampled data:]\")\n",
    "print(classification_report(y_test_u, clf_l1_under.predict(X_test_u)))\n",
    "\n",
    "print(\"[Final Train Classification Report L2 Regularization under-resampled data:]\")\n",
    "print(classification_report(y_train_u, clf_l2_under.predict(X_train_u)))\n",
    "\n",
    "print(\"[Final Test Classification Report L2 Regularization under-resampled data:]\")\n",
    "print(classification_report(y_test_u, clf_l2_under.predict(X_test_u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best sqrt Random Forest under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       161\n",
      "          1       1.00      1.00      1.00       160\n",
      "\n",
      "avg / total       1.00      1.00      1.00       321\n",
      "\n",
      "[Final Test Classification Report Best sqrt Random Forest under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.62      0.60        53\n",
      "          1       0.60      0.56      0.58        54\n",
      "\n",
      "avg / total       0.59      0.59      0.59       107\n",
      "\n",
      "[Final Training Classification Report Best log2 Random Forest under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       161\n",
      "          1       1.00      1.00      1.00       160\n",
      "\n",
      "avg / total       1.00      1.00      1.00       321\n",
      "\n",
      "[Final Test Classification Report Best log2 Random Forest under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.49      0.51        53\n",
      "          1       0.53      0.57      0.55        54\n",
      "\n",
      "avg / total       0.53      0.53      0.53       107\n",
      "\n",
      "[Final Training Classification Report Best None Random Forest under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       161\n",
      "          1       1.00      1.00      1.00       160\n",
      "\n",
      "avg / total       1.00      1.00      1.00       321\n",
      "\n",
      "[Final Test Classification Report Best None Random Forest under-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.58      0.57        53\n",
      "          1       0.58      0.56      0.57        54\n",
      "\n",
      "avg / total       0.57      0.57      0.57       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best sqrt Random Forest under-resampled data:]\")\n",
    "print(classification_report(y_train_u, clf_sr_under.predict(X_train_u)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best sqrt Random Forest under-resampled data:]\")\n",
    "print(classification_report(y_test_u, clf_sr_under.predict(X_test_u)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best log2 Random Forest under-resampled data:]\")\n",
    "print(classification_report(y_train_u, clf_lg_under.predict(X_train_u)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best log2 Random Forest under-resampled data:]\")\n",
    "print(classification_report(y_test_u, clf_lg_under.predict(X_test_u)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best None Random Forest under-resampled data:]\")\n",
    "print(classification_report(y_train_u, clf_no_under.predict(X_train_u)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best None Random Forest under-resampled data:]\")\n",
    "print(classification_report(y_test_u, clf_no_under.predict(X_test_u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(df.drop(['Home Win'],axis=1), df['Home Win'])\n",
    "\n",
    "df_smote = pd.DataFrame(X_smote)\n",
    "df_smote['Home Win'] = y_smote\n",
    "\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(df_smote.drop(['Home Win'],axis=1), \n",
    "                                              df_smote['Home Win'],random_state=1,stratify=df_smote['Home Win'])\n",
    "\n",
    "clf_l1_smote = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_l1_smote.fit(X_train_s,y_train_s)\n",
    "clf_l2_smote = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_l2_smote.fit(X_train_s,y_train_s)\n",
    "clf_sr_smote = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_sr_smote.fit(X_train_s,y_train_s)\n",
    "clf_lg_smote = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_lg_smote.fit(X_train_s,y_train_s)\n",
    "clf_no_smote = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_no_smote.fit(X_train_s,y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Train Classification Report L1 Regularization SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.64      0.64       227\n",
      "          1       0.64      0.63      0.64       227\n",
      "\n",
      "avg / total       0.64      0.64      0.64       454\n",
      "\n",
      "[Final Test Classification Report L1 Regularization SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.58      0.54        76\n",
      "          1       0.50      0.42      0.46        76\n",
      "\n",
      "avg / total       0.50      0.50      0.50       152\n",
      "\n",
      "[Final Train Classification Report L2 Regularization SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.63      0.62       227\n",
      "          1       0.62      0.60      0.61       227\n",
      "\n",
      "avg / total       0.62      0.62      0.62       454\n",
      "\n",
      "[Final Test Classification Report L2 Regularization SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.59      0.56        76\n",
      "          1       0.53      0.46      0.49        76\n",
      "\n",
      "avg / total       0.53      0.53      0.52       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Train Classification Report L1 Regularization SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_train_s, clf_l1_smote.predict(X_train_s)))\n",
    "\n",
    "print(\"[Final Test Classification Report L1 Regularization SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_test_s, clf_l1_smote.predict(X_test_s)))\n",
    "\n",
    "print(\"[Final Train Classification Report L2 Regularization SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_train_s, clf_l2_smote.predict(X_train_s)))\n",
    "\n",
    "print(\"[Final Test Classification Report L2 Regularization SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_test_s, clf_l2_smote.predict(X_test_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best sqrt Random Forest SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       227\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       454\n",
      "\n",
      "[Final Test Classification Report Best Random Forest SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.64      0.62        76\n",
      "          1       0.62      0.58      0.60        76\n",
      "\n",
      "avg / total       0.61      0.61      0.61       152\n",
      "\n",
      "[Final Training Classification Report Best log2 Random Forest SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       227\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       454\n",
      "\n",
      "[Final Test Classification Report Best log2 Random Forest SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.63      0.62        76\n",
      "          1       0.62      0.61      0.61        76\n",
      "\n",
      "avg / total       0.62      0.62      0.62       152\n",
      "\n",
      "[Final Training Classification Report Best None Random Forest SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       227\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       454\n",
      "\n",
      "[Final Test Classification Report Best None Random Forest SMOTE over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.63      0.62        76\n",
      "          1       0.62      0.61      0.61        76\n",
      "\n",
      "avg / total       0.62      0.62      0.62       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best sqrt Random Forest SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_train_s, clf_sr_smote.predict(X_train_s)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best Random Forest SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_test_s, clf_sr_smote.predict(X_test_s)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best log2 Random Forest SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_train_s, clf_lg_smote.predict(X_train_s)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best log2 Random Forest SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_test_s, clf_lg_smote.predict(X_test_s)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best None Random Forest SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_train_s, clf_no_smote.predict(X_train_s)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best None Random Forest SMOTE over-resampled data:]\")\n",
    "print(classification_report(y_test_s, clf_no_smote.predict(X_test_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=74, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "X_adasyn, y_adasyn = ADASYN().fit_sample(df.drop(['Home Win'],axis=1), df['Home Win'])\n",
    "\n",
    "df_adasyn = pd.DataFrame(X_adasyn)\n",
    "df_adasyn['Home Win'] = y_adasyn\n",
    "\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(df_adasyn.drop(['Home Win'],axis=1), \n",
    "                                              df_adasyn['Home Win'],random_state=1,stratify=df_adasyn['Home Win'])\n",
    "\n",
    "clf_l1_adasyn = LogisticRegression(penalty='l1', C=max_l1)\n",
    "clf_l1_adasyn.fit(X_train_a,y_train_a)\n",
    "clf_l2_adasyn = LogisticRegression(penalty='l2', C=max_l2)\n",
    "clf_l2_adasyn.fit(X_train_a,y_train_a)\n",
    "clf_sr_adasyn = RandomForestClassifier(max_features='sqrt',n_estimators=max_sr)\n",
    "clf_sr_adasyn.fit(X_train_a,y_train_a)\n",
    "clf_lg_adasyn = RandomForestClassifier(max_features='log2',n_estimators=max_lg)\n",
    "clf_lg_adasyn.fit(X_train_a,y_train_a)\n",
    "clf_no_adasyn = RandomForestClassifier(max_features=None,n_estimators=max_no)\n",
    "clf_no_adasyn.fit(X_train_a,y_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Train Classification Report L1 Regularization ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.56      0.57       217\n",
      "          1       0.59      0.62      0.61       227\n",
      "\n",
      "avg / total       0.59      0.59      0.59       444\n",
      "\n",
      "[Final Test Classification Report L1 Regularization ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.45      0.46        73\n",
      "          1       0.49      0.51      0.50        76\n",
      "\n",
      "avg / total       0.48      0.48      0.48       149\n",
      "\n",
      "[Final Train Classification Report L2 Regularization ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.56      0.57       217\n",
      "          1       0.60      0.63      0.61       227\n",
      "\n",
      "avg / total       0.59      0.59      0.59       444\n",
      "\n",
      "[Final Test Classification Report L2 Regularization ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.47      0.48        73\n",
      "          1       0.51      0.53      0.52        76\n",
      "\n",
      "avg / total       0.50      0.50      0.50       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Train Classification Report L1 Regularization ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_train_a, clf_l1_adasyn.predict(X_train_a)))\n",
    "\n",
    "print(\"[Final Test Classification Report L1 Regularization ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_test_a, clf_l1_adasyn.predict(X_test_a)))\n",
    "\n",
    "print(\"[Final Train Classification Report L2 Regularization ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_train_a, clf_l2_adasyn.predict(X_train_a)))\n",
    "\n",
    "print(\"[Final Test Classification Report L2 Regularization ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_test_a, clf_l2_adasyn.predict(X_test_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Training Classification Report Best sqrt Random Forest ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       217\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       444\n",
      "\n",
      "[Final Test Classification Report Best sqrt Random Forest ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.60      0.56        73\n",
      "          1       0.56      0.49      0.52        76\n",
      "\n",
      "avg / total       0.55      0.54      0.54       149\n",
      "\n",
      "[Final Training Classification Report Best log2 Random Forest ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       217\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       444\n",
      "\n",
      "[Final Test Classification Report Best log2 Random Forest ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.58      0.58        73\n",
      "          1       0.60      0.61      0.60        76\n",
      "\n",
      "avg / total       0.59      0.59      0.59       149\n",
      "\n",
      "[Final Training Classification Report Best None Random Forest ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       217\n",
      "          1       1.00      1.00      1.00       227\n",
      "\n",
      "avg / total       1.00      1.00      1.00       444\n",
      "\n",
      "[Final Test Classification Report Best None Random Forest ADASYN over-resampled data:]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.59      0.55        73\n",
      "          1       0.54      0.46      0.50        76\n",
      "\n",
      "avg / total       0.53      0.52      0.52       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Final Training Classification Report Best sqrt Random Forest ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_train_a, clf_sr_adasyn.predict(X_train_a)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best sqrt Random Forest ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_test_a, clf_sr_adasyn.predict(X_test_a)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best log2 Random Forest ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_train_a, clf_lg_adasyn.predict(X_train_a)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best log2 Random Forest ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_test_a, clf_lg_adasyn.predict(X_test_a)))\n",
    "\n",
    "print(\"[Final Training Classification Report Best None Random Forest ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_train_a, clf_no_adasyn.predict(X_train_a)))\n",
    "\n",
    "print(\"[Final Test Classification Report Best None Random Forest ADASYN over-resampled data:]\")\n",
    "print(classification_report(y_test_a, clf_no_adasyn.predict(X_test_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, some better scores.  It looks like a random forest classifier with estimators set to sqrt on random naive or SMOTE over-sampled data is the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for sqrt trees:\n",
      "    estimated coefficients         features\n",
      "5                 0.051913            H PK%\n",
      "17                0.051652            A PK%\n",
      "16                0.050779            A PP%\n",
      "12                0.049185  A Fenwick Close\n",
      "7                 0.048455            H sv%\n",
      "13                0.047871             A GF\n",
      "11                0.045369        H 5-5 F/A\n",
      "0                 0.044943  H Fenwick Close\n",
      "1                 0.044658             H GF\n",
      "4                 0.043426            H PP%\n",
      "18                0.042954            A sh%\n",
      "20                0.042153            A PDO\n",
      "3                 0.040318         H GlDiff\n",
      "23                0.040035        A 5-5 F/A\n",
      "6                 0.039778            H sh%\n",
      "8                 0.039516            H PDO\n",
      "2                 0.037470             H GA\n",
      "22                0.037161       A standing\n",
      "15                0.037151         A GlDiff\n",
      "9                 0.036719     H win streak\n",
      "14                0.035429             A GA\n",
      "19                0.033832            A sv%\n",
      "10                0.032714       H standing\n",
      "21                0.026516     A win streak\n"
     ]
    }
   ],
   "source": [
    "df_coef_rf_sr = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_sr_smote.feature_importances_,24)})\n",
    "df_coef_rf_sr = df_coef_rf_sr.reindex(df_coef_rf_sr['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for sqrt trees:')\n",
    "print(df_coef_rf_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficients for sqrt trees:\n",
      "    estimated coefficients         features\n",
      "12                0.049793  A Fenwick Close\n",
      "5                 0.049662            H PK%\n",
      "16                0.049161            A PP%\n",
      "1                 0.048433             H GF\n",
      "4                 0.047923            H PP%\n",
      "20                0.047812            A PDO\n",
      "0                 0.047439  H Fenwick Close\n",
      "2                 0.046190             H GA\n",
      "17                0.046009            A PK%\n",
      "23                0.045516        A 5-5 F/A\n",
      "14                0.044435             A GA\n",
      "6                 0.044186            H sh%\n",
      "19                0.043744            A sv%\n",
      "13                0.043271             A GF\n",
      "11                0.042551        H 5-5 F/A\n",
      "8                 0.041804            H PDO\n",
      "3                 0.041604         H GlDiff\n",
      "18                0.040974            A sh%\n",
      "7                 0.038499            H sv%\n",
      "15                0.033819         A GlDiff\n",
      "22                0.032740       A standing\n",
      "9                 0.026029     H win streak\n",
      "10                0.024664       H standing\n",
      "21                0.023744     A win streak\n"
     ]
    }
   ],
   "source": [
    "df_coef_rf_sr = pd.DataFrame({'features': df.drop(['Home Win'],axis=1).columns, 'estimated coefficients': np.reshape(clf_sr_over.feature_importances_,24)})\n",
    "df_coef_rf_sr = df_coef_rf_sr.reindex(df_coef_rf_sr['estimated coefficients'].sort_values(ascending=False).index)\n",
    "print('Estimated Coefficients for sqrt trees:')\n",
    "print(df_coef_rf_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt/8HnSFxeZa0IKgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19c9b160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_enc = OneHotEncoder()\n",
    "rf_lm = LogisticRegression()\n",
    "rf_enc.fit(clf_sr_over.apply(X_train_o))\n",
    "rf_lm.fit(rf_enc.transform(clf_sr_over.apply(X_train_o)), y_train_o)\n",
    "\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(clf_sr_over.apply(X_test_o)))[:, 1]\n",
    "\n",
    "fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test_o, y_pred_rf_lm)\n",
    "roc_auc = auc(fpr_rf_lm, tpr_rf_lm)\n",
    "\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHUZFJKkrt+1CZ/W8zPTfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b22e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision = average_precision_score(y_test_o, y_pred_rf_lm)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test_o, y_pred_rf_lm)\n",
    "\n",
    "plt.plot(recall, precision, color='darkorange', label='Precision Recall curve (area = %0.2f)' % average_precision)\n",
    "plt.plot([1, 0], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision Recall curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ1EzswL8P/F9fXm3Pr4nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19cc6f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "y_pred_cm = rf_lm.predict(rf_enc.transform(clf_sr_over.apply(X_test_o)))\n",
    "cnf_matrix = confusion_matrix(y_test_o, y_pred_cm)\n",
    "plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Home Win','Home Loss'], rotation=45)\n",
    "plt.yticks(tick_marks, ['Home Win','Home Loss'])\n",
    "\n",
    "fmt = '.0f'\n",
    "thresh = cnf_matrix.max() / 2.\n",
    "for i, j in itertools.product(range(2), range(2)):\n",
    "    plt.text(j, i, format(cnf_matrix[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
